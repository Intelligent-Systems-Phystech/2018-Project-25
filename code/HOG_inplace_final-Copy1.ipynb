{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### надо установить opencv для python, если не установлен.\n",
    "#### pip(pip3) install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример получения вектора признаков для изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее задаются параметры HOG-a, они выставлены как в статье.\n",
    "win_size = (64,128) \n",
    "block_size = (16,16)\n",
    "block_stride = (8,8)\n",
    "cell_size = (8,8)\n",
    "nbins = 9\n",
    "deriv_aperture = 1\n",
    "win_sigma = -1\n",
    "histogram_norm_type = 0\n",
    "l2_hys_threshold = 2.0000000000000001e-01\n",
    "gamma_correction = 0\n",
    "nlevels = 64\n",
    "hog = cv2.HOGDescriptor(win_size,block_size,block_stride,cell_size,nbins,deriv_aperture,win_sigma,\n",
    "                        histogram_norm_type,l2_hys_threshold,gamma_correction,nlevels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В базе INRIA размер изображений содержащих пешеходов  - 96х160(train) и 70x134(test). Человек расположен по центру.\n",
    "\n",
    "Вам нужно получить вектор признаков(дескриптор) для окна 64х128. По умолчанию compute() строит дескрипторы для всех окон размером 64х128 методом скользящего окна, с шагом win_stride по обеим осям. Но если вам нужно получить дескриптор только для одного окна 64х128, то у метода есть аргумент locations, в котором можно передать координаты верхнего левого угла интересующего вас окна (либо список таких координат)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, для изображений из train извлечение центрального окна 64х128 выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"crop_000010a.png\")\n",
    "height, width = img.shape[:2]\n",
    "locations = [((width-64)//2, (height-128)//2)]\n",
    "descriptor = hog.compute(img, locations=locations)\n",
    "descriptor.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображения фона(отрицательные примеры)  - изображения произвольного размера. В обучающей выборке их 1218. Авторы HOG для обучения выбирали рандомно 10 окон 64х128 из каждого изображения  - всего 12180 примеров, не содержащих пешеходов.\n",
    "\n",
    "Для этого, например, можно посчитать дескрипторы по всему изображению с win_stride=(4,4) и потом рандомно выбрать 10 из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие манипуляции вам надо проделать в цикле со всеми изображениями из train: для изображений из папки pos/ выбирать центральное окно, а из папки neg/ - 10 рандомных окон. Собрать их в np.array  или список, как вам будет удобно ну и в файл сохранить тоже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы получить ответ svm классификатора у класса HOGDescriptor есть метод detect(). Этот метод вам понадобится, чтобы строить кривые качества (DET кривые). Метод detect() работает похожим образом с compute().\n",
    "Только перед его использованием нужно задать веса SVM:\n",
    "\n",
    "    hog.setSVMDetector(hog.getDefaultPeopleDetector()) - используется веса из opencv для HOG, обученного на INRIA\n",
    "В остальном поробуйте сами с detect() разобраться. Если не получится - пишите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гайд по построению DET кривых позже добавлю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделим признаки на позитивных картинках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_stride = (8,8) #шаг скользящего окна в пикселях по ширине и высоте\n",
    "def imHOG(path_to_image):\n",
    "    img = cv2.imread(path_to_image)\n",
    "    height, width = img.shape[:2]\n",
    "    locations = [((width-64)//2, (height-128)//2)]\n",
    "    descriptor = hog.compute(img, locations=locations)\n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "images = open(\"pos.lst\")\n",
    "string = images.read()\n",
    "images = string.split(\"\\n\")\n",
    "for i in range(len(images)):\n",
    "    images[i] = images[i][10:]\n",
    "images = images[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for i in range(len(images)):\n",
    "    #desc_file.write(str(imHOG(images[i]))+\"\\n\")\n",
    "    descriptor_current = imHOG(images[i])\n",
    "    pos.append(descriptor_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = open(\"pos_test.lst\")\n",
    "string = images.read()\n",
    "images = string.split(\"\\n\")\n",
    "for i in range(len(images)):\n",
    "    images[i] = images[i][9:]\n",
    "images = images[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = []\n",
    "for i in range(len(images)):\n",
    "    #desc_file.write(str(imHOG(images[i]))+\"\\n\")\n",
    "    descriptor_current = imHOG(images[i])\n",
    "    pos_test.append(descriptor_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# На негативных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_stride = (8,8) #шаг скользящего окна в пикселях по ширине и высоте\n",
    "def imHOG_neg(path_to_image):\n",
    "    img = cv2.imread(path_to_image)\n",
    "    win_stride = (4,4)\n",
    "    #получаем дескрипторы изображения и приводим их к рамеру\n",
    "    #(кол-во окон на изображении)х(рамер дескриптора одного окна)\n",
    "    #рамер дексриптора для параметров HOG как в статье - 3780\n",
    "    descriptors = hog.compute(img, win_stride).reshape(-1,3780)\n",
    "    indexes = np.random.randint(descriptors.shape[0], size=10)\n",
    "    ten_random_samples = descriptors[indexes]\n",
    "    return ten_random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neg = open(\"neg.lst\")\n",
    "string_neg = images_neg.read()\n",
    "images_neg = string_neg.split(\"\\n\")\n",
    "for i in range(len(images_neg)):\n",
    "    images_neg[i] = images_neg[i][10:]\n",
    "images_neg = images_neg[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "for i in range(int(len(images_neg))):  #use less neg pictures\n",
    "    desc_hog = imHOG_neg(images_neg[i])\n",
    "    for i in range (len(desc_hog)):\n",
    "        neg.append(desc_hog[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neg = open(\"neg_test.lst\")\n",
    "string_neg = images_neg.read()\n",
    "images_neg = string_neg.split(\"\\n\")\n",
    "for i in range(len(images_neg)):\n",
    "    images_neg[i] = images_neg[i][9:]\n",
    "images_neg = images_neg[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test = []\n",
    "for i in range(int(len(images_neg))):   #use less neg_test pictures\n",
    "    desc_hog = imHOG_neg(images_neg[i])\n",
    "    for descriptor_current in desc_hog:\n",
    "        neg_test.append(descriptor_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решейпим полученные дескрипторы pos, pos_test, neg, neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12180 4530 2416 1132\n"
     ]
    }
   ],
   "source": [
    "print(len(neg),len(neg_test),len(pos),len(pos_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos)):\n",
    "    for j in range(3780):\n",
    "        pos[i][j] = pos[i][j][0]\n",
    "        \n",
    "for i in range(len(pos_test)):\n",
    "    for j in range(3780):\n",
    "        pos_test[i][j] = pos_test[i][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReshapeOneHOG(hog):\n",
    "    new_hog = []\n",
    "    for i in range(15):\n",
    "        new_hog.append([])\n",
    "        new_hog.append([])\n",
    "        for j in range(7):\n",
    "            cell = ReshapeCell(hog[i*36*7+36*j : i*36*7+36*(j+1)])\n",
    "            new_hog[2*i].append(cell[0])\n",
    "            new_hog[2*i+1].append(cell[1])\n",
    "            new_hog[2*i].append(cell[2])\n",
    "            new_hog[2*i+1].append(cell[3])\n",
    "    return new_hog\n",
    "            \n",
    "def ReshapeCell(cell):\n",
    "    c = []\n",
    "    c.append(cell[0:9])\n",
    "    c.append(cell[9:18])\n",
    "    c.append(cell[18:27])\n",
    "    c.append(cell[27:36])\n",
    "    return c\n",
    "\n",
    "def Reshape_in(Q):\n",
    "    RES=[]\n",
    "    for i in range(0,9):\n",
    "        a=[] #30 array of 14 elem\n",
    "        for j in range(0,15):\n",
    "            b=[] #14 elem\n",
    "            c=[]#14 elem\n",
    "            for k in range(0,7):\n",
    "                b.append(Q[2*j*14*9+k*36+i])\n",
    "                b.append(Q[2*j*14*9+k*36+i+18])\n",
    "                c.append(Q[2*j*14*9+k*36+i+9])\n",
    "                c.append(Q[2*j*14*9+k*36+i+27])\n",
    "            a.append(b)\n",
    "            a.append(c)\n",
    "        RES.append(a)\n",
    "    return RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout level 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomininvladislav/env/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6559\n",
      "Loss: 0.7484\n",
      "Loss: 0.0968\n",
      "Loss: 0.2865\n",
      "Accuracy 0.862 h 0.5\n",
      "Accuracy 0.856 h 0.567\n",
      "Accuracy 0.845 h 0.633\n",
      "Accuracy 0.825 h 0.7\n",
      "Dropout level 0.7\n",
      "Loss: 0.6513\n",
      "Loss: 0.7478\n",
      "Loss: 0.1435\n",
      "Loss: 0.2724\n",
      "Accuracy 0.865 h 0.5\n",
      "Accuracy 0.859 h 0.567\n",
      "Accuracy 0.851 h 0.633\n",
      "Accuracy 0.83 h 0.7\n",
      "Dropout level 0.7\n",
      "Loss: 0.6521\n",
      "Loss: 0.7134\n",
      "Loss: 0.1107\n",
      "Loss: 0.3003\n",
      "Accuracy 0.864 h 0.5\n",
      "Accuracy 0.861 h 0.567\n",
      "Accuracy 0.853 h 0.633\n",
      "Accuracy 0.834 h 0.7\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)    9x30x14\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 3, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(9, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(18, 36, kernel_size=5, stride=1, padding=1)\n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(36 * 14 * 6, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 32, 32) to (18, 32, 32)   9x30x14 to 18x30x14\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        #print(x.shape)\n",
    "        #to 18x14x6\n",
    "        x = F.relu(self.conv2(x))   #to 36x14x6\n",
    "        #print(x.shape)\n",
    "        #print(x.shape)\n",
    "        #Size changes from (18, 32, 32) to (18, 16, 16)  36x14x6 to 36x7x3\n",
    "        x = self.pool(x)\n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 36 * 14 * 6)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4608) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "for n in range(3):\n",
    "    print(\"Dropout level {}\".format(dropout))\n",
    "    model = SimpleCNN()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    for i in range(800):\n",
    "        # Forward pass\n",
    "        #xinput = torch.tensor(Reshape_in(neg[i]), dtype = torch.float32)\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        for j in range(3):\n",
    "            inputs.append(Reshape_in(neg[9*i+3*j]))\n",
    "            labels.append([0])\n",
    "            inputs.append(Reshape_in(neg[9*i+3*j+1]))\n",
    "            labels.append([0])\n",
    "            inputs.append(Reshape_in(neg[9*i+3*j+2]))\n",
    "            labels.append([0])\n",
    "            inputs.append(Reshape_in(pos[3*i+j]))\n",
    "            labels.append([1])\n",
    "        inputs = torch.from_numpy(np.array(inputs))\n",
    "        labels = torch.tensor(labels, dtype = torch.float32)\n",
    "        #print(inputs.shape, labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 200 == 0:\n",
    "            print ('Loss: {:.4f}' \n",
    "                   .format(loss.item()))\n",
    "        \n",
    "    #test\n",
    "    for h in np.linspace(0.5, 0.7, 4):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        acc = 0\n",
    "        for i in range(1100):\n",
    "\n",
    "            inputs.append(Reshape_in(neg[2*i]))\n",
    "            labels.append([0])\n",
    "            inputs.append(Reshape_in(neg[2*i+1]))\n",
    "            labels.append([0])\n",
    "            inputs.append(Reshape_in(pos[i]))\n",
    "            labels.append([1])\n",
    "        inputs = torch.from_numpy(np.array(inputs))\n",
    "        labels = torch.tensor(labels, dtype = torch.float32)\n",
    "        outputs = model(inputs)\n",
    "        for j in range(len(inputs)):\n",
    "            if outputs[j] > h:\n",
    "                if labels[j] == 1:\n",
    "                    acc += 1\n",
    "            else:\n",
    "                if labels[j] == 0:\n",
    "                    acc += 1\n",
    "        print(\"Accuracy {} h {}\".format(round(acc/len(labels), 3), round(h,3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomininvladislav/env/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.881 h 0.5\n",
      "Accuracy 0.884 h 0.567\n",
      "Accuracy 0.881 h 0.633\n",
      "Accuracy 0.873 h 0.7\n"
     ]
    }
   ],
   "source": [
    "for h in np.linspace(0.5, 0.7, 4):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    acc = 0\n",
    "    for i in range(1100):\n",
    "\n",
    "        inputs.append(Reshape_in(neg[2*i]))\n",
    "        labels.append([0])\n",
    "        inputs.append(Reshape_in(neg[2*i+1]))\n",
    "        labels.append([0])\n",
    "        inputs.append(Reshape_in(pos[i]))\n",
    "        labels.append([1])\n",
    "    inputs = torch.from_numpy(np.array(inputs))\n",
    "    labels = torch.tensor(labels, dtype = torch.float32)\n",
    "    outputs = model(inputs)\n",
    "    for j in range(len(inputs)):\n",
    "        if outputs[j] > h:\n",
    "            if labels[j] == 1:\n",
    "                acc += 1\n",
    "        else:\n",
    "            if labels[j] == 0:\n",
    "                acc += 1\n",
    "    print(\"Accuracy {} h {}\".format(round(acc/len(labels), 3), round(h,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
