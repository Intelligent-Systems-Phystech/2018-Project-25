{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### надо установить opencv для python, если не установлен.\n",
    "#### pip(pip3) install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример получения вектора признаков для изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее задаются параметры HOG-a, они выставлены как в статье.\n",
    "win_size = (64,128) \n",
    "block_size = (16,16)\n",
    "block_stride = (8,8)\n",
    "cell_size = (8,8)\n",
    "nbins = 9\n",
    "deriv_aperture = 1\n",
    "win_sigma = -1\n",
    "histogram_norm_type = 0\n",
    "l2_hys_threshold = 2.0000000000000001e-01\n",
    "gamma_correction = 0\n",
    "nlevels = 64\n",
    "hog = cv2.HOGDescriptor(win_size,block_size,block_stride,cell_size,nbins,deriv_aperture,win_sigma,\n",
    "                        histogram_norm_type,l2_hys_threshold,gamma_correction,nlevels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В базе INRIA размер изображений содержащих пешеходов  - 96х160(train) и 70x134(test). Человек расположен по центру.\n",
    "\n",
    "Вам нужно получить вектор признаков(дескриптор) для окна 64х128. По умолчанию compute() строит дескрипторы для всех окон размером 64х128 методом скользящего окна, с шагом win_stride по обеим осям. Но если вам нужно получить дескриптор только для одного окна 64х128, то у метода есть аргумент locations, в котором можно передать координаты верхнего левого угла интересующего вас окна (либо список таких координат)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, для изображений из train извлечение центрального окна 64х128 выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"crop_000010.png\")\n",
    "height, width = img.shape[:2]\n",
    "locations = [((width-64)//2, (height-128)//2)]\n",
    "descriptor = hog.compute(img, locations=locations)\n",
    "descriptor.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображения фона(отрицательные примеры)  - изображения произвольного размера. В обучающей выборке их 1218. Авторы HOG для обучения выбирали рандомно 10 окон 64х128 из каждого изображения  - всего 12180 примеров, не содержащих пешеходов.\n",
    "\n",
    "Для этого, например, можно посчитать дескрипторы по всему изображению с win_stride=(4,4) и потом рандомно выбрать 10 из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие манипуляции вам надо проделать в цикле со всеми изображениями из train: для изображений из папки pos/ выбирать центральное окно, а из папки neg/ - 10 рандомных окон. Собрать их в np.array  или список, как вам будет удобно ну и в файл сохранить тоже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы получить ответ svm классификатора у класса HOGDescriptor есть метод detect(). Этот метод вам понадобится, чтобы строить кривые качества (DET кривые). Метод detect() работает похожим образом с compute().\n",
    "Только перед его использованием нужно задать веса SVM:\n",
    "\n",
    "    hog.setSVMDetector(hog.getDefaultPeopleDetector()) - используется веса из opencv для HOG, обученного на INRIA\n",
    "В остальном поробуйте сами с detect() разобраться. Если не получится - пишите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гайд по построению DET кривых позже добавлю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделим признаки на позитивных картинках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_stride = (8,8) #шаг скользящего окна в пикселях по ширине и высоте\n",
    "def imHOG(path_to_image):\n",
    "    img = cv2.imread(path_to_image)\n",
    "    height, width = img.shape[:2]\n",
    "    locations = [((width-64)//2, (height-128)//2)]\n",
    "    descriptor = hog.compute(img, locations=locations)\n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "images = open(\"pos.lst\")\n",
    "string = images.read()\n",
    "images = string.split(\"\\n\")\n",
    "for i in range(len(images)):\n",
    "    images[i] = images[i][10:]\n",
    "images = images[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for i in range(len(images)):\n",
    "    #desc_file.write(str(imHOG(images[i]))+\"\\n\")\n",
    "    descriptor_current = imHOG(images[i])\n",
    "    pos.append(descriptor_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = open(\"pos_test.lst\")\n",
    "string = images.read()\n",
    "images = string.split(\"\\n\")\n",
    "for i in range(len(images)):\n",
    "    images[i] = images[i][9:]\n",
    "images = images[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = []\n",
    "for i in range(len(images)):\n",
    "    #desc_file.write(str(imHOG(images[i]))+\"\\n\")\n",
    "    descriptor_current = imHOG(images[i])\n",
    "    pos_test.append(descriptor_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# На негативных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_stride = (8,8) #шаг скользящего окна в пикселях по ширине и высоте\n",
    "def imHOG_neg(path_to_image):\n",
    "    img = cv2.imread(path_to_image)\n",
    "    win_stride = (4,4)\n",
    "    #получаем дескрипторы изображения и приводим их к рамеру\n",
    "    #(кол-во окон на изображении)х(рамер дескриптора одного окна)\n",
    "    #рамер дексриптора для параметров HOG как в статье - 3780\n",
    "    descriptors = hog.compute(img, win_stride).reshape(-1,3780)\n",
    "    indexes = np.random.randint(descriptors.shape[0], size=10)\n",
    "    ten_random_samples = descriptors[indexes]\n",
    "    return ten_random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neg = open(\"neg.lst\")\n",
    "string_neg = images_neg.read()\n",
    "images_neg = string_neg.split(\"\\n\")\n",
    "for i in range(len(images_neg)):\n",
    "    images_neg[i] = images_neg[i][10:]\n",
    "images_neg = images_neg[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "for i in range(int(len(images_neg)/20)):  #use less neg pictures\n",
    "    desc_hog = imHOG_neg(images_neg[i])\n",
    "    for i in range (len(desc_hog)):\n",
    "        neg.append(desc_hog[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_neg = open(\"neg_test.lst\")\n",
    "string_neg = images_neg.read()\n",
    "images_neg = string_neg.split(\"\\n\")\n",
    "for i in range(len(images_neg)):\n",
    "    images_neg[i] = images_neg[i][9:]\n",
    "images_neg = images_neg[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test = []\n",
    "for i in range(int(len(images_neg)/20)):   #use less neg_test pictures\n",
    "    desc_hog = imHOG_neg(images_neg[i])\n",
    "    for descriptor_current in desc_hog:\n",
    "        neg_test.append(descriptor_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решейпим полученные дескрипторы pos, pos_test, neg, neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 220 614 288\n"
     ]
    }
   ],
   "source": [
    "print(len(neg),len(neg_test),len(pos),len(pos_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos)):\n",
    "    for j in range(3780):\n",
    "        pos[i][j] = pos[i][j][0]\n",
    "        \n",
    "for i in range(len(pos_test)):\n",
    "    for j in range(3780):\n",
    "        pos_test[i][j] = pos_test[i][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReshapeOneHOG(hog):\n",
    "    new_hog = []\n",
    "    for i in range(15):\n",
    "        new_hog.append([])\n",
    "        new_hog.append([])\n",
    "        for j in range(7):\n",
    "            cell = ReshapeCell(hog[i*36*7+36*j : i*36*7+36*(j+1)])\n",
    "            new_hog[2*i].append(cell[0])\n",
    "            new_hog[2*i+1].append(cell[1])\n",
    "            new_hog[2*i].append(cell[2])\n",
    "            new_hog[2*i+1].append(cell[3])\n",
    "    return new_hog\n",
    "            \n",
    "def ReshapeCell(cell):\n",
    "    c = []\n",
    "    c.append(cell[0:9])\n",
    "    c.append(cell[9:18])\n",
    "    c.append(cell[18:27])\n",
    "    c.append(cell[27:36])\n",
    "    return c\n",
    "\n",
    "def Reshape_in(Q):\n",
    "    RES=[]\n",
    "    for i in range(0,9):\n",
    "        a=[] #30 array of 14 elem\n",
    "        for j in range(0,15):\n",
    "            b=[] #14 elem\n",
    "            c=[]#14 elem\n",
    "            for k in range(0,7):\n",
    "                b.append(Q[2*j*14*9+k*36+i])\n",
    "                b.append(Q[2*j*14*9+k*36+i+18])\n",
    "                c.append(Q[2*j*14*9+k*36+i+9])\n",
    "                c.append(Q[2*j*14*9+k*36+i+27])\n",
    "            a.append(b)\n",
    "            a.append(c)\n",
    "        RES.append(a)\n",
    "    return RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomininvladislav/env/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4607],\n",
      "        [0.5400]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.6168\n",
      "tensor([[0.3325],\n",
      "        [0.5893]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.4665\n",
      "tensor([[0.3061],\n",
      "        [0.5306]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.4996\n",
      "tensor([[0.3116],\n",
      "        [0.3005]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.7879\n",
      "tensor([[0.3649],\n",
      "        [0.8191]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.3268\n",
      "tensor([[0.2660],\n",
      "        [0.6155]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.3972\n",
      "tensor([[0.6288],\n",
      "        [0.6201]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.7344\n",
      "tensor([[0.2430],\n",
      "        [0.8023]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.2494\n",
      "tensor([[0.1842],\n",
      "        [0.3143]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.6805\n",
      "tensor([[0.6985],\n",
      "        [0.7346]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.7537\n",
      "tensor([[0.7585],\n",
      "        [0.3550]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 1.2283\n",
      "tensor([[0.4173],\n",
      "        [0.6952]], grad_fn=<SigmoidBackward>)\n",
      "Loss: 0.4518\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)    9x30x14\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 3, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(9, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = torch.nn.Conv2d(18, 36, kernel_size=3, stride=1, padding=1)\n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(36 * 15 * 7, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 32, 32) to (18, 32, 32)   9x30x14 to 18x30x14\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))   #to 36x30x14\n",
    "        #print(x.shape)\n",
    "        #Size changes from (18, 32, 32) to (18, 16, 16)  36x30x14 to 36x15x7\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 36 * 15 * 7)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4608) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "    \n",
    "\n",
    "model = SimpleCNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for i in range(600):\n",
    "    # Forward pass\n",
    "    #xinput = torch.tensor(Reshape_in(neg[i]), dtype = torch.float32)\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    inputs.append(Reshape_in(neg[i]))\n",
    "    labels.append([0])\n",
    "    inputs.append(Reshape_in(pos[i]))\n",
    "    labels.append([1])\n",
    "    inputs = torch.from_numpy(np.array(inputs))\n",
    "    labels = torch.tensor(labels, dtype = torch.float32)\n",
    "    #print(inputs.shape, labels)\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    #print(outputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(outputs)\n",
    "        print ('Loss: {:.4f}' \n",
    "               .format(loss.item()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomininvladislav/env/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.757 h 0.55\n",
      "Accuracy 0.755 h 0.555\n",
      "Accuracy 0.752 h 0.561\n",
      "Accuracy 0.75 h 0.566\n",
      "Accuracy 0.752 h 0.571\n",
      "Accuracy 0.745 h 0.576\n",
      "Accuracy 0.741 h 0.582\n",
      "Accuracy 0.739 h 0.587\n",
      "Accuracy 0.736 h 0.592\n",
      "Accuracy 0.73 h 0.597\n",
      "Accuracy 0.73 h 0.603\n",
      "Accuracy 0.736 h 0.608\n",
      "Accuracy 0.73 h 0.613\n",
      "Accuracy 0.725 h 0.618\n",
      "Accuracy 0.725 h 0.624\n",
      "Accuracy 0.727 h 0.629\n",
      "Accuracy 0.727 h 0.634\n",
      "Accuracy 0.727 h 0.639\n",
      "Accuracy 0.725 h 0.645\n",
      "Accuracy 0.72 h 0.65\n"
     ]
    }
   ],
   "source": [
    "for h in np.linspace(0.55, 0.65, 20):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    acc = 0\n",
    "    for i in range(220):\n",
    "\n",
    "        inputs.append(Reshape_in(neg[i]))\n",
    "        labels.append([0])\n",
    "        inputs.append(Reshape_in(pos[i]))\n",
    "        labels.append([1])\n",
    "    inputs = torch.from_numpy(np.array(inputs))\n",
    "    labels = torch.tensor(labels, dtype = torch.float32)\n",
    "    outputs = model(inputs)\n",
    "    for j in range(len(inputs)):\n",
    "        if outputs[j] > h:\n",
    "            if labels[j] == 1:\n",
    "                acc += 1\n",
    "        else:\n",
    "            if labels[j] == 0:\n",
    "                acc += 1\n",
    "    print(\"Accuracy {} h {}\".format(round(acc/len(labels), 3), round(h,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
